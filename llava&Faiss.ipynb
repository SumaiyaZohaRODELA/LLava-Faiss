{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNp8TnZZJaGPthD79YrnFf4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SumaiyaZohaRODELA/LLava-Faiss/blob/main/llava%26Faiss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "collapsed": true,
        "id": "msYIFvmc_2va",
        "outputId": "a257cec0-efa2-43b7-8472-df9d80ad06b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'faiss'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1977324962.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install transformers accelerate torch torchvision faiss-gpu llava\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from transformers import AutoProcessor, AutoModel\n",
        "\n",
        "# Load the LLaVA model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"liuhaotian/LLaVA-7b-delta-v0\"  # Replace with desired LLaVA model checkpoint\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name).to(device)\n",
        "model.eval()\n",
        "\n",
        "# Folder containing images\n",
        "image_folder = \"/content/image_dataset\"\n",
        "\n",
        "# Get all image paths\n",
        "image_files = glob(os.path.join(image_folder, \"*.jpg\"))\n",
        "\n",
        "# Randomly select 10 images for display\n",
        "random.seed(42)\n",
        "selected_images = random.sample(image_files, 10)\n",
        "\n",
        "# Display the selected images\n",
        "plt.figure(figsize=(20, 10))\n",
        "for i, img_path in enumerate(selected_images):\n",
        "    img = Image.open(img_path)\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Function to generate LLaVA embeddings\n",
        "def generate_llava_embeddings(images_path, processor, model, device):\n",
        "    image_paths = glob(os.path.join(images_path, \"*.jpg\"))\n",
        "    embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_path in image_paths:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            inputs = processor(images=[img], return_tensors=\"pt\").to(device)\n",
        "            outputs = model.get_image_features(**inputs)\n",
        "            embeddings.append(outputs.cpu().numpy().flatten())\n",
        "\n",
        "    return embeddings, image_paths\n",
        "\n",
        "# Generate embeddings using LLaVA\n",
        "embeddings, image_paths = generate_llava_embeddings(image_folder, processor, model, device)\n",
        "\n",
        "# Create FAISS index\n",
        "def create_faiss_index(embeddings, image_paths, output_path):\n",
        "    dimension = len(embeddings[0])\n",
        "    index = faiss.IndexFlatIP(dimension)  # Inner product for similarity\n",
        "    index = faiss.IndexIDMap(index)\n",
        "\n",
        "    vectors = np.array(embeddings).astype(np.float32)\n",
        "\n",
        "    # Add vectors to the index\n",
        "    index.add_with_ids(vectors, np.array(range(len(embeddings))))\n",
        "\n",
        "    # Save the index\n",
        "    faiss.write_index(index, output_path)\n",
        "    print(f\"Index created and saved to {output_path}\")\n",
        "\n",
        "    # Save image paths\n",
        "    with open(output_path + \".paths\", \"w\") as f:\n",
        "        for img_path in image_paths:\n",
        "            f.write(img_path + \"\\n\")\n",
        "\n",
        "    return index\n",
        "\n",
        "# Save FAISS index\n",
        "OUTPUT_INDEX_PATH = \"/content/vector_llava.index\"\n",
        "index = create_faiss_index(embeddings, image_paths, OUTPUT_INDEX_PATH)\n",
        "\n",
        "# Load FAISS index\n",
        "def load_faiss_index(index_path):\n",
        "    index = faiss.read_index(index_path)\n",
        "    with open(index_path + \".paths\", \"r\") as f:\n",
        "        image_paths = [line.strip() for line in f]\n",
        "    print(f\"Index loaded from {index_path}\")\n",
        "    return index, image_paths\n",
        "\n",
        "index, image_paths = load_faiss_index(OUTPUT_INDEX_PATH)\n",
        "\n",
        "# Function to retrieve similar images\n",
        "def retrieve_similar_images(query, processor, model, index, image_paths, top_k=3):\n",
        "    if isinstance(query, str):  # If query is a path\n",
        "        query = Image.open(query).convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(images=[query], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        query_features = model.get_image_features(**inputs).cpu().numpy().astype(np.float32)\n",
        "\n",
        "    distances, indices = index.search(query_features, top_k)\n",
        "    retrieved_images = [image_paths[int(idx)] for idx in indices[0]]\n",
        "\n",
        "    return query, retrieved_images\n",
        "\n",
        "# Visualize results\n",
        "def visualize_results(query, retrieved_images):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Display the query image\n",
        "    plt.subplot(1, len(retrieved_images) + 1, 1)\n",
        "    if isinstance(query, Image.Image):\n",
        "        plt.imshow(query)\n",
        "        plt.title(\"Query Image\")\n",
        "        plt.axis(\"off\")\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, f\"Query:\\n\\n '{query}'\", fontsize=16, ha=\"center\", va=\"center\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    # Display retrieved images\n",
        "    for i, img_path in enumerate(retrieved_images):\n",
        "        plt.subplot(1, len(retrieved_images) + 1, i + 2)\n",
        "        plt.imshow(Image.open(img_path))\n",
        "        plt.title(f\"Match {i + 1}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example query and retrieval\n",
        "query_image_path = \"/content/image_dataset/example.jpg\"  # Replace with your query image path\n",
        "query, retrieved_images = retrieve_similar_images(query_image_path, processor, model, index, image_paths, top_k=3)\n",
        "visualize_results(query, retrieved_images)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install transformers accelerate torch torchvision faiss-gpu llava\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from transformers import AutoProcessor, AutoModel"
      ],
      "metadata": {
        "id": "6FxT-Ad_NAk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the LLaVA model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"liuhaotian/LLaVA-7b-delta-v0\"  # Replace with desired LLaVA model checkpoint\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name).to(device)\n",
        "model.eval()\n",
        "\n",
        "# Folder containing images\n",
        "image_folder = \"/content/image_dataset\"\n",
        "\n",
        "# Get all image paths\n",
        "image_files = glob(os.path.join(image_folder, \"*.jpg\"))\n"
      ],
      "metadata": {
        "id": "MtE6cGHJNZr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly select 10 images for display\n",
        "random.seed(42)\n",
        "selected_images = random.sample(image_files, 10)\n",
        "\n",
        "# Display the selected images\n",
        "plt.figure(figsize=(20, 10))\n",
        "for i, img_path in enumerate(selected_images):\n",
        "    img = Image.open(img_path)\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Function to generate LLaVA embeddings\n",
        "def generate_llava_embeddings(images_path, processor, model, device):\n",
        "    image_paths = glob(os.path.join(images_path, \"*.jpg\"))\n",
        "    embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_path in image_paths:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            inputs = processor(images=[img], return_tensors=\"pt\").to(device)\n",
        "            outputs = model.get_image_features(**inputs)\n",
        "            embeddings.append(outputs.cpu().numpy().flatten())\n",
        "\n",
        "    return embeddings, image_paths"
      ],
      "metadata": {
        "id": "cqLlXjeTNjwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Generate embeddings using LLaVA\n",
        "embeddings, image_paths = generate_llava_embeddings(image_folder, processor, model, device)\n"
      ],
      "metadata": {
        "id": "3J8exEyQN7tC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}